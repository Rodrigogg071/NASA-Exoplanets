{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c210b9a-c95f-4790-af6e-837cf988950c",
   "metadata": {},
   "source": [
    "# Limpiar bases de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda41a9-333c-428b-b5f2-6c63d631f4fa",
   "metadata": {},
   "source": [
    "---\n",
    "### BASE DE DATOS - Kepler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd89e693-e756-4d9e-8dd7-547856c01bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('keplerdata.csv')\n",
    "\n",
    "# Quitamos los datos que no son atributos de los planetas u observaciones\n",
    "df = df.drop(columns=['rowid', 'kepid', 'kepoi_name', 'kepler_name', 'koi_vet_stat', 'koi_vet_date', 'koi_pdisposition', 'koi_disp_prov', 'koi_comment', 'koi_score', 'koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec'])\n",
    "\n",
    "# Quitamos candidatos dejando solo confirmados y falsos positivos\n",
    "df = df[df['koi_disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])]\n",
    "\n",
    "# Usamos one-hot encoding para koi_disposition\n",
    "df['is_exoplanet'] = (df['koi_disposition'] == 'CONFIRMED').astype(int)\n",
    "df = df.drop(columns=['koi_disposition'])\n",
    "\n",
    "# Take out columns with object data types\n",
    "for col, dtype in df.dtypes.items():\n",
    "    if not np.issubdtype(dtype, np.number):\n",
    "        df = df.drop(columns=col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c259289d-226a-45d8-8233-cdad34b48048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataframe limpio\n",
    "df.to_csv('kepler_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491b5ab-0bcc-4508-89e0-ca3642087dcb",
   "metadata": {},
   "source": [
    "---\n",
    "### BASE DE DATOS - Tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2161b369-c6d8-4e2a-89ff-ff32771acbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dt = pd.read_csv('tessdata.csv')\n",
    "\n",
    "# Quitamos los datos que no son atributos de los planetas u observaciones\n",
    "dt = dt.drop(columns=['rowid','toi','toipfx','tid','ctoi_alias','pl_pnum','rastr','ra','decstr','dec','st_pmra','st_pmdec','toi_created','rowupdate'])\n",
    "#print(dt.tfopwg_disp.value_counts(),dt.shape)\n",
    "\n",
    "# Eliminar todo lo que no nos diga si sí es o no  \n",
    "dt = dt[dt['tfopwg_disp'].isin(['FA', 'FP', 'KP', 'CP'])]\n",
    "\n",
    "# Convertimos FA y FP a NP, representando NO PLANETA; KP y CP a EP, representando ES PLANETA\n",
    "dt['tfopwg_disp'] = dt['tfopwg_disp'].replace({'KP': 'EP', 'CP': 'EP', 'FA': 'NP', 'FP': 'NP'})\n",
    "\n",
    "# Crear variable binaria (0 = NP, 1 = EP)\n",
    "dt['is_exoplanet'] = (dt['tfopwg_disp'] == 'EP').astype(int)\n",
    "\n",
    "# Eliminar la columna original si ya no la necesitas\n",
    "dt = dt.drop(columns=['tfopwg_disp'])\n",
    "\n",
    "# Asegurar que es_exoplaneta sea numérica (por si acaso)\n",
    "dt['is_exoplanet'] = pd.to_numeric(dt['is_exoplanet'])\n",
    "\n",
    "#print(dt.head())\n",
    "#print(dt.es_exoplaneta.value_counts(),dt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a22d2c52-030e-4fa9-b826-f72b42561c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataframe limpio\n",
    "dt.to_csv('tess_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60d478-ad1b-482f-bee3-00a5e1437173",
   "metadata": {},
   "source": [
    "---\n",
    "### BASE DE DATOS - K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8baf5a5-0abd-4034-bea9-8191bfb7eff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: k2_cleaned.csv | filas=2630 | cols=7\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# K2: Limpieza mínima y dataset etiquetado\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "FNAME = \"k2pandc_2025.10.05_02.35.26.csv\"\n",
    "OUT   = \"k2_cleaned.csv\"\n",
    "\n",
    "# 1) Carga (detección simple de separador)\n",
    "def detect_sep(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if not line.startswith(\"#\") and line.strip():\n",
    "                return \"\\t\" if \"\\t\" in line else \",\"\n",
    "    return \",\"\n",
    "\n",
    "sep = detect_sep(FNAME)\n",
    "df = pd.read_csv(FNAME, sep=sep, comment=\"#\", low_memory=False)\n",
    "\n",
    "# 2) Etiqueta binaria desde 'disposition'\n",
    "disp_cols = [c for c in df.columns if \"disposition\" in c.lower()]\n",
    "disp_col = disp_cols[0]\n",
    "\n",
    "df[disp_col] = df[disp_col].astype(str).str.upper().str.strip()\n",
    "is_conf = df[disp_col].str.contains(\"CONFIRMED\", na=False)\n",
    "is_fp   = df[disp_col].str.contains(\"FALSE POSITIVE\", na=False) | df[disp_col].str.contains(\"REFUTED\", na=False)\n",
    "\n",
    "# nos quedamos solo con confirmados o falsos positivos\n",
    "df = df.loc[ is_conf | is_fp ].copy()\n",
    "\n",
    "# crear etiqueta y remover columna original\n",
    "df[\"is_exoplanet\"] = np.where(is_conf.loc[df.index], 1, 0)\n",
    "df.drop(columns=[disp_col], inplace=True)\n",
    "\n",
    "# 3) Variables relevantes\n",
    "keep_cols = [\n",
    "    \"disposition\", \"pl_orbper\", \"st_rad\", \"sy_dist\",\n",
    "    \"sy_vmag\", \"sy_jmag\", \"sy_gaiamag\", \"is_exoplanet\"\n",
    "]\n",
    "keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "df = df[keep_cols].copy()\n",
    "\n",
    "# 4) Definir X, y\n",
    "X = df.drop(columns=[\"is_exoplanet\"])\n",
    "y = df[\"is_exoplanet\"]\n",
    "\n",
    "# 5) Guardar dataset limpio\n",
    "df.to_csv(OUT, index=False)\n",
    "print(f\"Guardado: {OUT} | filas={len(df)} | cols={df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b6069a-9895-4cd5-b12d-3fa5ec0619cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar dataset limpio\n",
    "    # clean = X.copy()\n",
    "    # clean[\"is_exoplanet\"] = y\n",
    "    # clean.to_csv(\"k2_cleaned.csv\", index=False)\n",
    "#print(\"Archivo guardado: k2_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1159e6-b152-41a5-a2f6-c8f7ade3737a",
   "metadata": {},
   "source": [
    "# Creación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d4c9e-43dc-46eb-a58a-3e73fa27fd41",
   "metadata": {},
   "source": [
    "## MODELO BOOSTING - KEPLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a54417e-1cc9-48ad-8768-1e5e5d2a4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# =======================================\n",
    "# 1️⃣ CARGA\n",
    "# =======================================\n",
    "df = pd.read_csv(\"kepler_cleaned.csv\", sep=None, engine=\"python\")\n",
    "\n",
    "# =======================================\n",
    "# 2️⃣ SEPARACIÓN DE VARIABLES (split primero)\n",
    "# =======================================\n",
    "X = df.drop(columns=[\"is_exoplanet\"])\n",
    "y = df[\"is_exoplanet\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Columnas numéricas (derivadas del TRAIN)\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea38891-575f-476b-8185-041ae621e7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boost_kepler.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", KNNImputer(n_neighbors=5)),\n",
    "    (\"sc\", StandardScaler()),\n",
    "])\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", preprocess),\n",
    "    (\"clf\", GradientBoostingClassifier(n_estimators=500,learning_rate=0.05,max_depth=3,random_state=42)\n",
    ")\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Guarda el pipeline entero\n",
    "joblib.dump(pipe, \"boost_kepler.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a04e5-669c-4802-b4b9-eed640784002",
   "metadata": {},
   "source": [
    "## MODELO BOOSTING - TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df19a0e-ce8d-4568-b3c3-5e662bde2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"tess_cleaned.csv\", sep=None, engine=\"python\")\n",
    "\n",
    "# ==============================\n",
    "# 1) SEPARAR VARIABLES\n",
    "# ==============================\n",
    "X = df.drop(columns=[\"is_exoplanet\"])\n",
    "y = df[\"is_exoplanet\"]\n",
    "\n",
    "# Split primero (¡sin preprocesar antes!)s\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 2) DEFINIR COLUMNAS NUMÉRICAS\n",
    "# ==============================\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f133ef38-146b-4fda-8d91-4ba040a8f496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boost_tess.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", KNNImputer(n_neighbors=5)),\n",
    "    (\"sc\", StandardScaler()),\n",
    "])\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", preprocess),\n",
    "    (\"clf\", GradientBoostingClassifier(n_estimators=500,learning_rate=0.05,max_depth=3,random_state=42)\n",
    ")\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Guarda el pipeline entero\n",
    "joblib.dump(pipe, \"boost_tess.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd95560-9f5f-4995-99ac-dd63d60cc08d",
   "metadata": {},
   "source": [
    "## MODELO BOOSTING - K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c6c80e1-7ee4-4a57-8f32-cf932bc0fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# =======================================\n",
    "# CARGA\n",
    "# =======================================\n",
    "df = pd.read_csv(\"k2_cleaned.csv\", engine=\"python\")\n",
    "\n",
    "# =======================================\n",
    "# IMPUTACIÓN + ESCALADO \n",
    "# =======================================\n",
    "\n",
    "# Etiqueta y predictores (usa 'is_exoplanet' del CSV limpio)\n",
    "X = df.drop(columns=[\"is_exoplanet\"])\n",
    "y = df[\"is_exoplanet\"]\n",
    "\n",
    "# Split primero\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Columnas numéricas basadas en el TRAIN\n",
    "num_cols = X_train.select_dtypes(include=[\"float64\", \"int64\", \"float32\", \"int32\"]).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f81624e3-5bd3-4b63-8401-3d13da2ceb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boost_k2.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Supón que ya tienes X_train, y_train y num_cols definidos\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", KNNImputer(n_neighbors=5)),\n",
    "    (\"sc\", StandardScaler()),\n",
    "])\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", preprocess),\n",
    "    (\"clf\", GradientBoostingClassifier(n_estimators=500,learning_rate=0.05,max_depth=3,random_state=42)\n",
    ")\n",
    "])\n",
    "\n",
    "# Entrena una sola vez\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Guarda el pipeline entero\n",
    "joblib.dump(pipe, \"boost_k2.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
